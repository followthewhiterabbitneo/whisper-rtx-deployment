#!/bin/bash
# Setup Ollama on RHEL8 with Gemma model

echo "=== Setting up Ollama on RHEL8 ==="

# Install Ollama
echo "Installing Ollama..."
curl -fsSL https://ollama.ai/install.sh | sh

# Check if installed
if command -v ollama &> /dev/null; then
    echo "✓ Ollama installed successfully"
    ollama --version
else
    echo "✗ Ollama installation failed"
    echo "Try manual install:"
    echo "wget https://github.com/ollama/ollama/releases/download/v0.1.38/ollama-linux-amd64"
    echo "chmod +x ollama-linux-amd64"
    echo "sudo mv ollama-linux-amd64 /usr/local/bin/ollama"
    exit 1
fi

# Start Ollama service
echo ""
echo "Starting Ollama service..."
ollama serve > /tmp/ollama.log 2>&1 &
OLLAMA_PID=$!
sleep 5

# Create Modelfile for your Gemma GGUF
MODEL_PATH="/moneyball/whisper-rtx-deployment/models/gemma-2-9b-it-Q5_K_M.gguf"

echo ""
echo "Creating Modelfile for Gemma..."
cat > Modelfile << EOF
FROM $MODEL_PATH

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 2048
EOF

# Create the model in Ollama
echo ""
echo "Creating Gemma model in Ollama..."
ollama create gemma2-legal -f Modelfile

# Test the model
echo ""
echo "Testing Gemma model..."
ollama run gemma2-legal "Summarize: Customer called about loan status."

# Create legal summary script
cat > gemma_legal_summary_ollama.py << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
Legal summary generator using Ollama Gemma
"""
import subprocess
import json
from pathlib import Path

def query_gemma_ollama(prompt):
    """Query Gemma using Ollama CLI"""
    cmd = ["ollama", "run", "gemma2-legal", "--format", "json"]
    
    full_prompt = f"""You are a legal assistant. Create a professional legal summary.
{prompt}

Legal Summary:"""
    
    try:
        result = subprocess.run(
            cmd,
            input=full_prompt,
            capture_output=True,
            text=True
        )
        if result.returncode == 0:
            return result.stdout.strip()
        else:
            return f"Error: {result.stderr}"
    except Exception as e:
        return f"Error: {e}"

def process_transcript(transcript_file):
    """Process a transcript file and generate legal summary"""
    transcript = Path(transcript_file).read_text()
    
    # Truncate if too long
    if len(transcript) > 2000:
        transcript = transcript[:2000] + "..."
    
    prompt = f"Summarize this call transcript:\n{transcript}"
    
    summary = query_gemma_ollama(prompt)
    
    # Save summary
    output_file = Path(transcript_file).with_suffix('.legal_summary.txt')
    output_file.write_text(f"Legal Summary (Generated by Gemma 2 9B)\n{'='*50}\n{summary}")
    
    print(f"Summary saved to: {output_file}")
    return summary

if __name__ == "__main__":
    # Test with Eric Rawlins transcript
    test_transcript = "Eric Rawlins called about loan modification status. Application submitted June 1st."
    print("Test summary:", query_gemma_ollama(f"Summarize: {test_transcript}"))
PYTHON_EOF

chmod +x gemma_legal_summary_ollama.py

echo ""
echo "=== Setup Complete ==="
echo "Ollama is running with your Gemma model!"
echo ""
echo "To use:"
echo "1. Quick test: ollama run gemma2-legal 'Summarize: Customer called about loan'"
echo "2. Python script: ./gemma_legal_summary_ollama.py"
echo "3. Stop Ollama: kill $OLLAMA_PID"
echo ""
echo "Ollama log: /tmp/ollama.log"